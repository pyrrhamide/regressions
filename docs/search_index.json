[["reg-lin.html", "1 Régressions linéaires 1.1 Moindres Carrés Ordinaires - hypothèses 1.2 Régressions linéaires simple et multiple 1.3 Régression linéaire de probabilité", " 1 Régressions linéaires 1.1 Moindres Carrés Ordinaires - hypothèses \\[Y_i = \\beta_0 + \\beta_iX_i + \\epsilon_i\\] pour \\(i = 1,...,n\\) \\(Y_i\\) est la variable dépendante, selon lobservation \\(i\\). \\(\\beta_0\\) est le point à lorigine/lintercept. \\(\\beta_i\\) est le coefficient de la variable explicative \\(X_i\\). \\(\\epsilon_i\\) est lerreur-type/le résidu, propre à lindividu \\(i\\). Le modèle de régression linéaire est estimé à partir de la méthode des Moindres Carrés Ordinaires (MCO). Cette méthode consiste à minimiser la somme des carrés des écarts1, écarts pondérés dans le cas multidimensionnel, entre chaque point du nuage de régression et son projeté, parallèlement à laxe des ordonnées, sur la droite de régression. On estime les paramètres \\(\\beta_i\\) de telle sorte que \\(\\Sigma_i\\epsilon_i\\) soit minimale (donc on cherche à réduire la place du hasard dans notre modèle). Important pour les cours suivants  les MCO suivent 6 hypothèses: Linéarité des paramètres (relation linéaire entre \\(Y\\) et \\(X\\), \\(Y\\) continue). Absence dautocorrélation des variables explicatives (il ne faut pas quune variable soit le multiple dune autre). Homoscédasticité (la variance des erreur-types est la même pour toutes les observations). Absence dautocorrélation des résidus (les résidus sont indépendants les uns des autres. Sinon voudrait dire que les observations sont corrélées entre elles). Normalité des résidus (moyenne des résidus égale à 0). Absence de corrélation entre les variables explicatives et le résidu dans le modèle théorique. La violation dune ou plusieurs de ces hypothèses implique un changement de variables, ou lutilisation dun différent modèle de régression. 1.2 Régressions linéaires simple et multiple La régression linéaire prend comme variable dépendante/à expliquer (\\(var\\_dep\\)) une variable quantitative continue, telle que la température, le PIB, mon envie de dormir progressive pendant les cours dEQles variables explicatives/indépendantes peuvent être quantitatives ou qualitatives. Tu lis les coefficients de la régression de la manière suivante : toutes choses égales par ailleurs, pour une unité de \\(x_i\\) (\\(var\\_indep\\)) en plus, \\(y_i\\) (\\(var\\_dep\\)) augmente/diminue de \\(\\beta_i\\) (le coefficient) ou bien toutes choses égales par ailleurs, pour un changement de catégorie de \\(x\\), \\(y\\) augmente/diminue de {coefficient}. # régression linéaire simple (une seule var indep) model_1 &lt;- lm(var_dep ~ var_indep, data = d) summary(model_1) # pour observer le modèle (coefficients, p.value [Pr(&gt;|z|)], deviance, etc.) # pour visualiser la régression plot(d$var_dep, d$var_indep) abline(model_1) On interprète seulement les coefficients statistiquement significatifs (avec des petites étoiles) : un coefficient statistiquement significatif veut dire quon peut rejeter lhypothèse que la variable explicative ninfluence pas la variable à expliquer. Toutefois ça peut être intéressant de dire je suis surprise que cette variable nest pas deffet, ce qui va à lencontre de mon hypothèse/de la littérature, blabla. # régression linéraire multiple (plusieurs var indeps) sans interaction model_2 &lt;- lm(var_dep ~ var_indep1 + var_indep2 + var_indep3, data = d) # si on souhaite obtenir un modèle pondéré model_3 &lt;- lm(var_dep ~ var_indep1 + var_indep2 + var_indep3, data = d, weights = poids) # on peut aussi &quot;mettre à jour&quot; le modèle 2 model_3 &lt;- update(model_2, weights=poids) # régression linéaire multiple avec interaction model_4 &lt;- lm(var_dep ~ var_indep1 + var_indep2 + var_indep3 + var_indep1*var_indep2, data = d, weights = poids) # ou model_4 &lt;- update(model_3, . ~ . + var_indep1*var_indep2) Le coefficient dinteraction sadditionne aux coefficients de base des catégories qui correspondent à ce nouveau coef. Par exemple, on interagit le PIB avec le taux de natalité. Tu obtiens les résultats suivants : - \\(\\beta_1\\) le coefficient du PIB; - \\(\\beta_2\\) le coefficient du taux de natalité; - \\(\\beta_3\\) le coefficient de linteraction entre les deux variables. Pour obtenir le vrai effet du PIB et du taux de natalité sur la \\(var\\_dep\\), tu additionnes ces trois coefficients. 1.3 Régression linéaire de probabilité Cependant en sociologie, il est rare davoir une variable continue à expliquer. On a généralement des variables catégorielles (qui peuvent être une variable quanti transformée en variable quali). Le modèle de régression linéaire ne fonctionne plus dans ce cas, on se tourne vers le modèle de régression linéaire de probabilité qui prend comme variable dépendante une variable dichotomique2. La ligne R reste la même, seule la nature de la \\(var\\_dep\\) change. Tu lis le coefficient de cette manière : toutes choses égales par ailleurs, pour une unité de plus/pour un changement de catégorie de \\(x\\), la probabilité que lévènement \\(y=1\\) se passe augmente (resp. diminue)3. # régression linéaire de probabilité model_5 &lt;- lm(var_dep_dicho ~ var_indep1 + var_indep2 + var_indep3, data = d, weights = poids) Ecart par rapport à la ligne de régression. On a un nuage de point, une droite qui traverse ces points. La plupart des points ne seront pas sur la droite: la distance entre la droite et le point est le résidu. Du coup on viole la première hypothèse des MCO sur la linéarité des paramètres. Je suis pas sûre de la lecture exacte du coefficient, mais en gros ça fonctionne comme ça. "]]
