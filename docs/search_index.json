[["index.html", "Toutes les méthodes du master Présentation Le site Mea culpa Aide moi à taider", " Toutes les méthodes du master KF Présentation Le site Ce site, crée avec bookdown, a pour vocation de répertorier (presque) toutes les méthodes enseignées lors de notre master, et de taider à comprendre les bases. Nos cours ne sont pas généreux en sujet-verbe-COD, jessaie dy remédier ici. Jai simplifié autant que je pouvais les 4 premiers chapitres, tandis que le reste est principalement copié-collé des diapos des profs. Jallégerais peut-être ça plus tard. Tu trouveras en tête des chapitres 3 à 6 les packages propres à chaque cours, au lieu dau milieu du code. Jai également présenter les fonctions avec du vocabulaire plus descriptif que celui de la documentation officielle, puis ai arrêté petit-à-petit parce que ça devenait trop répétitif. Jespère ajouter des chapitres sur lanalyse géométrique des données, sur le datamining, et sur RMarkdown. Si tu as des demandes particulières, nhésite pas à men parler. En attendant, tu as tous les trucs de régressions ici. Mea culpa Jai fait le maximum pour que ça ne soit pas trop compliqué, je continue à améliorer tout ça ponctuellement, et je mexcuse si certains points ne sont pas assez clairs. Il y a des choses qui sont limpides pour moi et que, par conséquent, je nexplicite pas plus que ça. À la base, ce site était un fichier Rmd unique, pour moi, et était donc personnalisé pour mes connaissances. Aide moi à taider Cest pour ça que je tencourage fortement à contribuer à ce site en envoyant des suggestions (clique sur le bouton bloc-note en haut, ça temmènera sur le fichier Rmd de la page), ou même en étant un.e auteur.e officiel.le, pour que ce soit agréable pour toi et pour toute la promo. Jai adoré bosser dessus, mais résumer 2 ans de cours (avec des notes incomplètes) seule demande beaucoup defforts. Un dernier petit mot: si tu es coincé.e, google google google. Ou regarde les scripts des profs. Ou lis la documentation officielle. "],["stat-desc.html", "Statistiques descriptives", " Statistiques descriptives ## Packages à charger, nécessaire pour l&#39;analyse ## library(tidyverse) library(questionr) # pour effectuer des tris à plat et des tris croisés library(survey) # pour travailler avec des données pondérées library(glm2) # pour effectuer régressions logistiques library(nnet) # pour effectuer régressions logistiques polytomiques library(GDAtools) On importe les données puis on les observe. Ci-dessous jai mis des fonctions de bases avec un dataframe qui nexiste pas. Jai lu récemment que ce nétait pas très data analyst que de faire View(data_frame) pour visualiser les données. A la place, il faut utiliser la console (ou ton script) pour en apprendre le plus possible sur tes données1 dim(d) # nb lignes (observations) et nb colonnes (variables-normalement) names(d) str(d) # type des variables sum(is.na(d)) summary(d$x) unique(d$x) levels(d$x) # au choix table(d$x,d$y,useNA=&quot;ifany&quot;) prop.table(table(d$x,d$y,useNA=&quot;ifany&quot;)) rprop(table(d$x,d$y)) # corrélation cor(d$x,d$y,method=&#39;pearson&#39;) # test du khi-deux chisq.test(d$x, d$y) Toutefois, ces mesures ne suffisent pas pour décider de faire une régression linéaire ! Il faut également visualiser les données, pour être sûr.e que la répartition soit bien linéaire. Après, ce nest pas la fin du monde si tu décides de visualiser ta base entièrement. Je le fais. "],["nettoyage.html", "Nettoyage des données", " Nettoyage des données Il faut nettoyer pour éviter le plus derreur au niveau des commandes. Ca veut dire gérer les non-réponses, changer la nature des variables, changer leurs noms, réordonner les modalités Jadhère à la philosophie dHadley Wickham: une ligne = une observation, et une colonne = une variable. Dans le cas des données de panel, ça veut dire avoir plusieurs lignes par année pour une unité (individu, pays). Je ne mattarde pas dessus, mais si tes données sont semi-propres, intéresse toi à la fonction tidyr::pivot_longer. On nettoie les données en prévision des régressions futures. On dichotomise notamment les modalités des variables explicatives pour les régressions logistiques (codage disjonctif complet - on nous recommandait ça pendant le M1, mais R peut dichotomiser les modalités des variables comme un grand, tant que les variables sont du type facteur). Par exemple : On a une variable couleur qui prend comme \\(n\\) modalités 1 = bleu ; 2 = blanc ; 3 = rouge. On va la transformer en \\(n-1\\) (dans ce cas, 2) variables binaires, une indiquant si cest bleu (0 = non ; 1 = oui), une indiquant si cest blanc (idem). Il ny a pas besoin de créer une troisième variable dichotomique car, par défaut, une observation qui est à bleu = 0 et blanc = 0 serait à rouge = 1: # méthode du prof d$bleu[d$couleur==1] &lt;- 1 d$bleu[d$couleur!=1] &lt;- 0 d$blanc[d$couleur==2] &lt;- 1 d$blanc[d$couleur!=2] &lt;- 0 # plus propre avec tidyverse d &lt;- d %&gt;% mutate(bleu = ifelse(couleur==1,1,0), blanc = ifelse(couleur==2,1,0)) En gros, pour une variable à \\(n\\) modalités/catégories que tu veux utiliser dans une régression, tu génères \\(n-1\\) variables dichotomiques. Toutefois, ce nest pas du tout nécessaire parce que les fonctions de régressions de R dichotomisent automatiquement les modalités des variables catégorielles. Si tu as une variable à \\(n\\) modalités, tu peux aussi choisir de regrouper deux modas ensemble et de les coder en 0 et le reste des modas en 1. Important : une fois que tu as déterminé toutes tes variables explicatives, transforme les en facteur et change leur niveau/ordre. Les régressions fonctionnent avec des variables facteurs. Exemple : prenons une variable des mentions du bac qui est à lorigine sous la forme caractère. Sous le format caractère, les mentions seront rangées par ordre alphabétique et non par ordre hiérachique évident. Il revient à toi dordonner les modalités comme tu le souhaites. d$mentions &lt;- factor(d$mentions, levels=c(&#39;Sans mention&#39;,&#39;Passable&#39;,&#39;Assez bien&#39;,&#39;Bien&#39;,&#39;Très bien&#39;)) # Note: il existe d&#39;autres fonctions du pkg forcats pour accélérer l&#39;ordonnancement des modalités. Je te laisse les découvrir, moi j&#39;ai pas le time, j&#39;ai curling sur gazon. "],["reg-lin.html", "1 Régressions linéaires 1.1 Moindres Carrés Ordinaires - hypothèses 1.2 Régressions linéaires simple et multiple 1.3 Régression linéaire de probabilité", " 1 Régressions linéaires 1.1 Moindres Carrés Ordinaires - hypothèses \\[Y_i = \\beta_0 + \\beta_iX_i + \\epsilon_i\\] pour \\(i = 1,...,n\\) \\(Y_i\\) est la variable dépendante, selon lobservation \\(i\\). \\(\\beta_0\\) est le point à lorigine/lintercept. \\(\\beta_i\\) est le coefficient de la variable explicative \\(X_i\\). \\(\\epsilon_i\\) est lerreur-type/le résidu, propre à lindividu \\(i\\). Le modèle de régression linéaire est estimé à partir de la méthode des Moindres Carrés Ordinaires (MCO). Cette méthode consiste à minimiser la somme des carrés des écarts2, écarts pondérés dans le cas multidimensionnel, entre chaque point du nuage de régression et son projeté, parallèlement à laxe des ordonnées, sur la droite de régression. On estime les paramètres \\(\\beta_i\\) de telle sorte que \\(\\Sigma_i\\epsilon_i^2\\) soit minimale (donc on cherche à réduire la place du hasard dans notre modèle). Important pour les cours suivants  les MCO suivent 6 hypothèses: Linéarité des paramètres (relation linéaire entre \\(Y\\) et \\(X\\), \\(Y\\) continue). Absence dautocorrélation des variables explicatives (il ne faut pas quune variable soit le multiple dune autre, autrement les logiciels danalyse les confondent et cest pas top). Homoscédasticité des erreurs-types (la variance des erreur-types est la même pour toutes les observations). Absence dautocorrélation des résidus (les résidus sont indépendants les uns des autres. Sinon voudrait dire que les observations sont corrélées entre elles). Normalité des résidus (moyenne des résidus égale à 0). Absence de corrélation entre les variables explicatives et le résidu dans le modèle théorique (risque de sur/sous-estimation du coefficient de la variable explicative). La violation dune ou plusieurs de ces hypothèses implique un changement de variables, ou lutilisation dun différent modèle de régression. 1.2 Régressions linéaires simple et multiple La régression linéaire prend comme variable dépendante/à expliquer (\\(var\\_dep\\)) une variable quantitative continue, telle que la température, le PIB, mon envie de dormir progressive pendant les cours dEQles variables explicatives/indépendantes peuvent être quantitatives ou qualitatives. Tu lis les coefficients de la régression de la manière suivante : toutes choses égales par ailleurs, pour une unité de \\(x_i\\) (\\(var\\_indep\\)) en plus, \\(y_i\\) (\\(var\\_dep\\)) augmente/diminue de \\(\\beta_i\\) (le coefficient) ou bien toutes choses égales par ailleurs, pour un changement de catégorie de \\(x\\), \\(y\\) augmente/diminue de {coefficient}. # régression linéaire simple (une seule var indep) model_1 &lt;- lm(var_dep ~ var_indep, data = d) summary(model_1) # pour observer le modèle (coefficients, p.value [Pr(&gt;|z|)], deviance, etc.) # pour visualiser la régression plot(d$var_dep, d$var_indep) abline(model_1) On interprète seulement les coefficients statistiquement significatifs (avec des petites étoiles) : un coefficient statistiquement significatif veut dire quon peut rejeter lhypothèse que la variable explicative ninfluence pas la variable à expliquer. Toutefois ça peut être intéressant de dire je suis surpris.e que cette variable nest pas deffet, ce qui va à lencontre de mon hypothèse/de la littérature, blabla. # régression linéraire multiple (plusieurs var indeps) sans interaction model_2 &lt;- lm(var_dep ~ var_indep1 + var_indep2 + var_indep3, data = d) # si on souhaite obtenir un modèle pondéré model_3 &lt;- lm(var_dep ~ var_indep1 + var_indep2 + var_indep3, data = d, weights = poids) # on peut aussi &quot;mettre à jour&quot; le modèle 2 model_3 &lt;- update(model_2, weights=poids) # régression linéaire multiple avec interaction model_4 &lt;- lm(var_dep ~ var_indep1 + var_indep2 + var_indep3 + var_indep1*var_indep2, data = d, weights = poids) # ou model_4 &lt;- update(model_3, . ~ . + var_indep1*var_indep2) Le coefficient dinteraction sadditionne aux coefficients de base des catégories qui correspondent à ce nouveau coef. Par exemple, on interagit le PIB avec le taux de natalité. Tu obtiens les résultats suivants : \\(\\beta_1\\) le coefficient du PIB; \\(\\beta_2\\) le coefficient du taux de natalité; \\(\\beta_3\\) le coefficient de linteraction entre les deux variables. Pour obtenir le vrai effet du PIB et du taux de natalité sur la \\(var\\_dep\\), tu additionnes ces trois coefficients. 1.3 Régression linéaire de probabilité Pas correct, please disregard. Je corrige ça un jour. Note KF: voir ici pour compléter cette partie. Cependant en sociologie, il est rare davoir une variable continue à expliquer. On a généralement des variables catégorielles (qui peuvent être une variable quanti transformée en variable quali). Le modèle de régression linéaire ne fonctionne plus dans ce cas, on se tourne vers le modèle de régression linéaire de probabilité qui prend comme variable dépendante une variable dichotomique3. La ligne R reste la même, seule la nature de la \\(var\\_dep\\) change. Tu lis le coefficient de cette manière : toutes choses égales par ailleurs, pour une unité de plus/pour un changement de catégorie de \\(x\\), la probabilité que lévènement \\(y=1\\) se passe augmente (resp. diminue)4. # régression linéaire de probabilité model_5 &lt;- lm(var_dep_dicho ~ var_indep1 + var_indep2 + var_indep3, data = d, weights = poids) Ecart par rapport à la ligne de régression. On a un nuage de point, une droite qui traverse ces points. La plupart des points ne seront pas sur la droite: la distance entre la droite et le point est le résidu. Du coup on viole la première hypothèse des MCO sur la linéarité des paramètres. Je suis pas sûre de la lecture exacte du coefficient, mais en gros ça fonctionne comme ça. "],["reg-log.html", "2 Régressions logistiques 2.1 Régression logistique dichotomique 2.2 Régression logistique polytomique 2.3 Odds ratio", " 2 Régressions logistiques \\[logit(p) = log(\\frac{p}{1-p}) = \\beta_0 + \\sum_{j=1}^n \\beta_j X_{ij}\\] 2.1 Régression logistique dichotomique Pour une raison qui méchappe parce que je suis pas très fraîche, et tout simplement parce que je ne sais pas, on nutilise pas le modèle de régression linéaire de probabilité mais plutôt le modèle de régression logistique dichotomique. Pour les coefficients, on parlera de leffet du logit de probabilité que lévènement \\(y=1\\) se passe. On lance dabord le modèle de régression logistique à effets principaux, ou modèle de lindépendance : model_6 &lt;- glm(var_dep_dicho ~ var_indep1 + var_indep2 + var_indep3, data = d, weights = poids, family=binomial) Petite note sur le modèle de lindépendance totale : cest le modèle dans lequel tu balances tes variables explicatives sans interaction ou association, pour vérifier si les variables sont indépendantes entre elles, càd si  toutes choses égales par ailleurs  elles expliquent le phénomène à elles seules. Cependant, en socio, cest inimaginable que la classe sociale et le sexe agissent indépendamment lun de lautre, cest pour cette raison quon choisirait dinteragir ces variables entre elles. Pour vérifier si les variables sont indépendantes, tu regardes la residual deviance : si cest un nb énorme5, direction interaction-ville. # On met le modèle précédent à jour, en ajoutant des interactions. # pour une unique interaction model_7 &lt;- update(model_6, . ~ . + var_indep1*var_indep2) # pour interagir toutes les modalités de toutes les variables entre elles (interaction d&#39;ordre 2) model_8 &lt;- glm(var_dep_dicho ~ (var_indep1 + var_indep2 + var_indep3)^2, data = d, weights = poids, family=binomial) # pour interagir var_indep3 avec var_indep1 et var_indep2 [plus rapide que d&#39;écrire individuellement chaque interaction] model_9 &lt;- glm(var_dep_dicho ~ (var_indep1 + var_indep2)*var_indep3, data = d, weights = poids, family=binomial) # le modèle saturé (interaction d&#39;ordre 3 - toutes les interactions inférieures seront automatiquement là) model_10 &lt;- glm(var_dep_dicho ~ var_indep1*var_indep2*var_indep3,d,binomial) Le coefficient dinteraction sadditionne aux coefficients de base des catégories qui correspondent à ce nouveau coef. Par exemple, on interagit le sexe et le niveau de diplôme. Tu obtiens les résultats suivants : \\(\\beta_1\\) le coefficient pour sexe=1 (par exemple les filles); \\(\\beta_2\\) le coefficient du niveau de diplôme=1 (par exemple ceux qui ont obtenu une licence); \\(\\beta_3\\) le coefficient de linteraction entre les deux modalités. Pour obtenir le vrai effet de lobtention de la licence chez les filles sur la \\(var\\_dep\\), tu additionnes ces trois coefficients. Tu ne peux pas additionner le coefficient dinteraction pour les garçons avec licence, ou les filles sans licence. On veut voir si lajout de ces interactions est statistiquement significative (il y a la méthode qui suit, et une autre quon a vu plus tard). On effectue un test statistique sur linteraction, qui est distribué comme une loi du khi-deux, sur la différence de vraisemblance entre les modèles dindépendance et dinteraction. test &lt;- 2*(logLik(model_7)-logLik(model_6)) [1] # Entre ces deux modèles, il n&#39;y a qu&#39;une variable/modalité en plus, donc le degré de liberté est de 1 (je sais plus pourquoi). 1-pchisq(test,1) # (test,1), 1 étant le ddl. On obtient la probabilité que linteraction soit due au hasard. Si la valeur est inférieure ou égale à 0.10, on peut rejeter lhypothèse que linteraction soit due au hasard. 2.2 Régression logistique polytomique On a vu la régression logistique dichotomique. Mais pour une variable à expliquer qui contient plus de 2 modalités, on utilise le modèle de régression logistique polytomique. Imaginons que tu ais une variable \\(n\\) à expliquer avec des modalités 1 (modalité de référence), 2 et 3, et une variable \\(m\\) explicative avec des modalités A (modalité de ref), B et C. Tu interpréteras un coefficient ainsi : toutes choses égales par ailleurs, le logit de probabilité que lévènement \\(y=2\\) se produise, par rapport à lévènement \\(y=1\\), pour le groupe B augmente/diminue par rapport au groupe A. Cest une phrase assez dégueulasse. Toutefois cest un modèle de régression sur lequel on ne sest pas attardés lannée dernière donc je nen dis pas plus. Voici la commande pour lancer le modèle. model_10 &lt;- multinom(var_dep_poly ~ var_indep1 + var_indep2 + var_indep3, data=d, weights=poids) Si je me souviens bien, on ne sest pas attardé dessus parce que la fonction était cassée. Le package qessmasteR (en cours de développement) contient la fonction multi_mpl pour effectuer ce type de régression. 2.3 Odds ratio Passons à un truc dont on parle beaucoup en classe : les odds ratio. Les coefficients dune régression logistique ne sont pas évidents à interpréter (dans le sens où cest long de dire le logit de probabilité et que humainement, cest moche). On passe donc les coefficients logit par la fonction exponentielle, ce qui nous donne les odds ratio (OR). Un OR est la chance quun évènement \\(y=1\\) plutôt que \\(y=0\\)6 se passe pour une condition B, par rapport à ce que cet évènement \\(y=1\\) se passe pour une condition A (condition de ref). LOR est compris entre \\([0;+\\infty[\\). Pour un OR compris entre 0 et 1, la chance que le truc se passe pour un groupe B est en fait moindre que la chance quil se passe pour le groupe A, mais on dira quand même lévènement a 0.14 fois plus de chance de se passer pour le groupe B que pour le groupe A. Après, il va de soi que multiplier quelque chose par une valeur inférieure à 1 diminue le résultat. Pour obtenir les coefficients de la régression en OR, on utilise la commande suivante : exp(model_7$coefficients) # ou model_7$coefficients %&gt;% exp # au choix Les odds ratios se calculent également à partir des effectifs attendus, estimés par le modèle (model$fitted.values). Imaginons quon ait le tableau de contingence suivant, avec les effectifs estimés: Je suis à lheure Je suis en retard Il y a une feuille sur les rails \\(m_{1,1}\\) \\(m_{1,2}\\) RAS sur les rails \\(m_{2,1}\\) \\(m_{2,2}\\) La formule pour calculer lOR est la suivante: \\[ OR = \\frac{m_{1,1} \\times m_{2,2}}{m_{2,1} \\times m_{1,2}} \\] Là jai calculé lodds ratio/la chance que je sois à lheure sachant quil y a une feuille sur les rails. Il existe trois relations selon la valeur de lOR: OR &lt; 1: indique une relation négative entre deux variables (moins de chance que). OR = 1: indique une relation neutre entre deux variables (même chance que). OR &gt; 1: indique une relation positive entre les deux variables (plus de chance que). Pour obtenir lodds ratio inverse de celui quon a dans les mains, on peut diviser 1 par lOR. Attention à linterprétation! Plus sérieusement, si la valeur dépasse largement le seuil critique de la distribution du khi-deux (selon les degrés de libertés of course), on rejette lhypothèse de lindépendance statistique entre les variables. Vu quil reste quand même un pâté de déviance à expliquer, on introduit des interactions dans le modèle de lindépendance totale. à vérifier. "],["log-lin.html", "3 Régression log-linéaire 3.1 Modèle de lindépendance 3.2 Modèles avec interaction 3.3 Analyse de deviance 3.4 Odds ratio", " 3 Régression log-linéaire Soit un modèle log-linéaire avec trois variables A, B et C ayant chacune des modalités indexées respectivement par les indices \\(i\\), \\(j\\) et \\(k\\). La table de contingence contient les effectifs notés \\(m_{ijk}\\). Un modèle log-linéaire est une régression sur le log des effectifs dans chacun des cases du tableau de contingence, soit : \\[log(m_{ijk}) = constante + \\\\ margeA_i + margeB_j + margeC_k + \\\\ interactionAB_{ij} + interactionAC_{ik} + interactionBC_{jk} + \\\\ interactionABC_{ijk}\\] Un modèle log-linéaire contient toujours la constante et toutes les marges du tableau de contingence. Puis on peut ensuite ajouter des interactions. Il sagit quasiment toujours de modèles hiérarchiques, càd que, dès lors que lon met une interaction dordre n entre un sous-groupe de variables, toutes les interactions dordre inférieur à n entre les variables de ce sous-groupe sont également incluses dans le modèle. La constante permet destimer leffectif total de la table. Les coefficients \\(margeA_i\\) permettent destimer les effectifs de la \\(i^{ème}\\) ligne de la table, qui correspond à la \\(i^{ème}\\) modalité de A. Résumons les différents modèles de régression quon a vu jusquà maintenant : - régression linéaire simple/multiple : variable à expliquer quantitative, toutes les variables sont dans leurs unités de bases. - régression linéaire de probabilité : variable à expliquer dichotomique, toutes les variables sont dans leurs unités de bases. - régression logistique dichotomique/polytomique : variable à expliquer dichotomique, toutes les variables sont mises à léchelle logarithmique (\\(log(x)\\)). On passe maintenant à la régression log-linéaire, que je résume (rapidement et sans doute pas correctement) ainsi : la variable à expliquer est mise à léchelle logarithmique, les variables explicatives restent dans leurs unités de bases. On fait un tri croisé avec ces variables. Ce modèle mesure lindépendance (ou non) statistique. Jutilise maintenant lexemple des admissions à Berkeley : \\(admissions*département*sexe\\). # Les packages et fonctions library(vcd) library(vcdExtra) library(DescTools) library(tidyverse) # source(&quot;f.util.cours.R&quot;) fonction créée par le prof stat_ajust &lt;- function(...) { list_glm &lt;- enquos(...) noms &lt;- as.character(list_glm) %&gt;% map_chr(~str_sub(.x, start = 2)) list_glm &lt;- map(list_glm, rlang::eval_tidy) return(map2_dfr(list_glm, noms, ~ tibble( model = .y, G2 = .x$deviance, ddl = .x$df.residual, p.value.G2 = 1 - pchisq(.x$deviance, .x$df.residual), dissimilarity = sum(abs(.x$y - .x$fitted.values)) / sum(.x$y) / 2, AIC = .x$aic, BIC = AIC(.x, k = log(sum(.x$y))) ))) } # il me semble que cette fonction ne fonctionne pas avec les données pondérées. utilise stat_ajust2 dans ce cas, dispo dans son fichier. # La base Berkeley &lt;- UCBAdmissions %&gt;% as.data.frame() 3.1 Modèle de lindépendance # Modèle log-linéaire # Modèle de l&#39;indépendance totale M0 &lt;- glm(Freq ~ Gender + Admit + Dept, family = poisson, data = UCBAdmissions) # La différence avec la reg logit est la famille de la distribution. Pour la reg logit, on avait family=binomial, ici on a family=poisson. summary(M0) Call: glm(formula = Freq ~ Gender + Admit + Dept, family = poisson, data = UCBAdmissions) Deviance Residuals: Min 1Q Median 3Q Max -18.170 -7.719 -1.008 4.734 17.153 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 5.37111 0.03964 135.498 &lt; 2e-16 *** GenderFemale -0.38287 0.03027 -12.647 &lt; 2e-16 *** AdmitRejected 0.45674 0.03051 14.972 &lt; 2e-16 *** DeptB -0.46679 0.05274 -8.852 &lt; 2e-16 *** DeptC -0.01621 0.04649 -0.349 0.727355 DeptD -0.16384 0.04832 -3.391 0.000696 *** DeptE -0.46850 0.05276 -8.879 &lt; 2e-16 *** DeptF -0.26752 0.04972 -5.380 7.44e-08 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for poisson family taken to be 1) Null deviance: 2650.1 on 23 degrees of freedom Residual deviance: 2097.7 on 16 degrees of freedom AIC: 2272.7 Number of Fisher Scoring iterations: 5 # Residual deviance = 2097.7 on 16 degrees of freedom. Il reste beaucoup trop dinformation à expliquer que le modèle de lindépendance totale nexplique pas, trop pour établir lindépendance entre les variables. On décide alors de mettre à jour le modèle en ajoutant des interactions. 3.2 Modèles avec interaction # une interaction prise en compte M1_GD &lt;- update(M0, . ~ . + Gender:Dept) # choix du département est genré M1_GA &lt;- update(M0, . ~ . + Gender:Admit) # admission discriminante en fonction du sexe M1_AD &lt;- update(M0, . ~ . + Admit:Dept) # départements plus ou moins selectifs # deux interactions M2_GD.GA &lt;- update(M1_GD, . ~ . + Admit:Gender) M2_GD.AD &lt;- update(M1_GD, . ~ . + Admit:Dept) M2_AD.GA &lt;- update(M1_AD, . ~ . + Admit:Gender) # trois interactions d&#39;ordre 2 M3 &lt;- update(M2_GD.AD, . ~ . + Gender:Admit) # une interaction d&#39;ordre 3 = modèle saturé M4 &lt;- update(M0, . ~ . + Gender*Admit*Dept) # sélectivité du département varie selon le sexe Tous les modèles sont stockés dans la mémoire de R, on veut maintenant voir le gain ou la perte dinformation offert.e par chaque nouveau modèle. 3.3 Analyse de deviance anova(M0, M1_GA, M1_AD, M1_GD, M2_AD.GA, M2_GD.GA, M2_GD.AD, M3, M4) Analysis of Deviance Table Model 1: Freq ~ Gender + Admit + Dept Model 2: Freq ~ Gender + Admit + Dept + Gender:Admit Model 3: Freq ~ Gender + Admit + Dept + Admit:Dept Model 4: Freq ~ Gender + Admit + Dept + Gender:Dept Model 5: Freq ~ Gender + Admit + Dept + Admit:Dept + Gender:Admit Model 6: Freq ~ Gender + Admit + Dept + Gender:Dept + Gender:Admit Model 7: Freq ~ Gender + Admit + Dept + Gender:Dept + Admit:Dept Model 8: Freq ~ Gender + Admit + Dept + Gender:Dept + Admit:Dept + Gender:Admit Model 9: Freq ~ Gender + Admit + Dept + Gender:Admit + Gender:Dept + Admit:Dept + Gender:Admit:Dept Resid. Df Resid. Dev Df Deviance 1 16 2097.67 2 15 2004.22 1 93.45 3 11 1242.35 4 761.87 4 11 877.06 0 365.29 5 10 1148.90 1 -271.84 6 10 783.61 0 365.29 7 6 21.74 4 761.87 8 5 20.20 1 1.53 9 0 0.00 5 20.20 En regardant la dernière colonne, Deviance, on peut voir que les modèles 3 (les départements sont sélectifs) et 7 (les départements sont sélectifs et genrés) sont ceux grâce auxquels on a gagné le plus dinformation. Le modèle 3 indique linteraction qui explique le mieux la variance du tableau. Le modèle 7 est le best overall, ayant le moins de residual deviance, cest donc le modèle qui expliquerait le mieux la variance des données. Visiblement, la sélectivité des départements et leurs compositions sont des éléments importants à inclure dans la régression. Pourquoi ne pas prendre le modèle 9 ? Parce que le modèle 9 est saturé (toutes les combinaisons dinteractions sont dans le modèle, cest logique que toutes les données soient expliquées.) Revenons en au modèle 7. Comment peut-on sassurer que le gain dinfo est réel et pas dû au hasard (nest pas du bruit)? stat_ajust(M0, M1_GA, M1_AD, M1_GD, M2_AD.GA, M2_GD.GA, M2_GD.AD, M3, M4) # A tibble: 9 x 7 model G2 ddl p.value.G2 dissimilarity AIC BIC &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 M0 2.10e+ 3 16 0 2.60e- 1 2273. 2324. 2 M1_GA 2.00e+ 3 15 0 2.57e- 1 2181. 2239. 3 M1_AD 1.24e+ 3 11 0 2.13e- 1 1427. 1511. 4 M1_GD 8.77e+ 2 11 0 1.69e- 1 1062. 1146. 5 M2_AD.GA 1.15e+ 3 10 0 1.89e- 1 1336. 1426. 6 M2_GD.GA 7.84e+ 2 10 0 1.56e- 1 971. 1061. 7 M2_GD.AD 2.17e+ 1 6 0.00135 1.64e- 2 217. 332. 8 M3 2.02e+ 1 5 0.00114 1.67e- 2 217. 339. 9 M4 -3.38e-14 0 1 1.81e-15 207. 361. Regardons la colonne p.value.G2: une \\(p.value\\) de 0 est suspicieux, on ne prend pas en compte les modèles qui ont cette valeur ; Dissimilarity : proportion des observations mal classées. Le plus bas, le mieux. De ce fait, le modèle 7 est toujours le meilleur, ce qui est confirmé par son BIC qui est le plus faible de tous les modèles (jsp ce que ça mesure par contre.) Les coefficients dun modèle log-linéaire avec interaction peuvent être obtenus avec une régression logistique dichotomique. 3.4 Odds ratio Les coefficients ne sont pas interprétables directement, on utilise les odds ratio. "],["mod-id2.html", "4 Modélisation et identification causale, vol. 2 4.1 Approches expérimentales 4.2 Variables instrumentales 4.3 Econométrie des panels", " 4 Modélisation et identification causale, vol. 2 library(lfe) library(AER) library(survival) library(plm) library(lmtest) Je mets immédiatement les lignes de codes complètes ici. Il est fortement recommandé dutiliser la fonction felm() du package lfe pour toutes les techniques de ce cours, mais il est aussi possible dutiliser lm(), plm::plm() ou ivreg() pour de petites vérifications. La fonction felm() fait tout: les effets fixes, les effets aléatoires, les variables instrumentales, la clusterisation des erreurs-typesits the GOAT . ## La syntaxe des commandes principales ## felm(y ~ x1 + x2 | f1 + f2 | x3 + x4 | clu1 + clu2, data, weights, ...) # soit felm(var_dep ~ vars_indep | vars_effet_fixe | vars_instrumentales | vars_cluster_se, data, weights, ...) # si on veut seulement clusteriser les erreurs-types, on remplace # vars_effet_fixe et/ou vars_instrumentales par 0 felm(y ~ x1+x2 |0|0|clu1+clu2, data, weights, ...) # Pour les variables instrumentales - plus détaillé plus tard ivreg(y ~ x1 + x2 | z1 + z2, data, weights, ...) # soit ivreg(var_dep ~ var_endo + vars_indep | vars_exo_inst, data, weights, ...) # Pour les données de panel/effets fixes et tout ça plm(y ~ x1 + x2, index=c(&quot;var_indiv&quot;,&quot;var_temps&quot;), data, model = &quot;pooling&quot;) # voir la documentation pour les autres &quot;model&quot; et également les &quot;effect&quot;. 4.1 Approches expérimentales Groupe traité v groupe de contrôle. Pour connaître leffet du traitement, groupe traité diffère du groupe de contrôle seulement du traitement. 4 méthodes autour de cette idée: Expériences aléatoires contrôlées Expériences naturelles Différences de différences Régression par discontinuité 4.1.1 Expériences aléatoires contrôlées 2 exemples dexpériences aléatoires contrôlées: Expérience à essais randomisés contrôlés (randomised controlled trials (RCT) experiments) Expériences par questionnaire (randomised survey experiment) On prend un groupe qui rempli les mêmes caractéristiques (du type la promo du M2) à qui on applique un ou des traitements différents pour voir si les résultats sur une variable à expliquer diffèrent. Il y a deux directrices. On est allés voir lune ou lautre lannée dernière (dans le contexte de lexpérience aléatoire, disons quon les a choisi aléatoirement) et cest celle quon a vu qui est devenue notre interlocutrice principale (cest la ventilation aléatoire il me semble). Du coup, au sein de la promo (normalement uniforme mais tmtc que nan), on est soumis à un traitement différent selon la dir. Y a-t-il une différence de notes/de moral en fonction de la personne ? Dans cette situation que je viens de décrire, il devrait normalement y avoir un troisième groupe (groupe de contrôle - indépendance du traitement) sans interlocutrice. Pourquoi lexpérience randomisée ? Ventilation aléatoire assure que toutes les caractéristiques des individus (à la fois observées et surtout inobservées) vont être ventilées de manière équiprobable dans les groupes traités ou de contrôle. Lestimation nest plus biaisée par une variable confondante (hétérogénéité inobservée) Simplification considérable de la statistique intensité indiquée par la différence (ou le rapport) des moyennes significativité test de différence de moyennes ou de proportions [préciser le.s tests en question] Expérience aléatoire versus échantillon aléatoire échantillon aléatoire: établir des statistiques représentatives dune population \\(\\rightarrow\\) validité externe [définir ce que cest]. expérience aléatoire: ventilation aléatoire dun échantillon \\(\\rightarrow\\) validité interne [définir]. Lexpérience et ses aveugles Simple aveugle: le patient ne sait pas dans quel groupe il est (traité ou placebo) Double aveugle: le patient et la personne qui administre le traitement ne savent pas dans quel groupe se trouve le patient Triple aveugle: le patient, la personne qui administre le traitement et le statisticien ne savent pas dans quel groupe se trouve le patient. Lexpérience aléatoire en sciences sociales est rarement une expérience avec placebo et en aveugle. Le placebo, qui doit avoir la forme, le goût, etc. du placebo, nexiste pas toujours [trouver exemple]. Il y a souvent 2 groupes: un qui fait lobjet dune intervention et un qui ne reçoit rien. Il importe alors dêtre vigilent à faire une analyse en intention de traiter (intention to treat) plutôt que traitement réalisé (treatment on treated). [parler des limites techniques et conceptuelles?] 4.1.2 Expériences naturelles Situation naturelle qui ressemble à une situation expérimentale (ventilation aléatoire ou quasi-aléatoire dune population entre groupe traité et groupe de contrôle) sans avoir été construite à des fins expérimentales. Tirages au sort comme procédure dallocation (e.g. jurés en tribunal, distribution dans les chambres dinternat, répartition entre ceux qui ont dû présenter leur problématique de mémoire le jour 1 et le jour 2) Autres exemples : jeux aléatoires et loteries, phénomène aléatoires ou quasi aléatoires (sexe de lenfant/mois de naissance), recrutement académique [EXEMPLE!!!] 4.1.3 Différences-de-différences (differences-in-differences) On reprend lopposition groupe traité versus groupe de contrôle des expériences contrôlées. Toutefois, on nest pas sûr que les groupes de contrôles soient vraiment similaires en tout point excepté le traitement. On va faire une hypothèse plus faible: la différence entre traitement et contrôle est constante dans le temps. différence prétraitement est la différence liée aux inobservables différence post-traitement est la différence liée aux inobservables + effet causal différence de différence est leffet causal (diff post - diff pré) Différence standard : \\[Traité - Contrôle \\\\ T_1 - C_1\\] Différence de différence est lestimation : \\[DiD = (Traité_{post}-Traité_{anté})-(Contrôle_{post}-Contrôle_{anté}) \\\\ DiD = (T_1-T_0)-(C_1-C_0)\\] Notations classiques : - \\(T_1=\\mu_{11}\\) ; \\(T_0=\\mu_{10}\\) ; \\(C_1=\\mu_{01}\\) ; \\(C_0=\\mu_{00}\\) - Diff-in-Diff = \\((\\mu_{11}-\\mu_{01})-(\\mu_{10}-\\mu_{00})\\) Estimations économétriques Quand on a un panel : - on mesure sur les mêmes individus, lavant et laprès, - on estime lévolution, - \\(\\Delta y_i = \\beta_0 + \\beta_1 GT + \\epsilon_i\\) ou GT est le groupe traité, - \\(\\Delta y_i\\) est lévolution/la variation de \\(y_i\\) - \\(\\beta_0\\) est lintercept [nom spécial en DiD?] - \\(\\beta_1\\) est lestimateur DiD. - \\(\\epsilon_i\\) est le terme derreur [distribution spéciale?] Quand on na pas de panel : - les individus avant et après ne sont pas les mêmes, - \\(y_{it} = \\beta_0 + \\beta_1 GT + \\beta_2 t + \\beta_3 t \\times GT + \\epsilon_{it}\\), - \\(\\beta_3\\) est lestimateur DiD. Portées et limites Hypothèses fortes: la différence entre le traitement et le contrôle serait restée constante en labsence de traitement. ou redit autrement, la différence de différence est unniquement due au traitement et non à un autre changement intervenu dans le groupe traité entre la période 1 et 2. Si on dispose de plus de deux périodes, on peut faire une vérification graphique. 4.1.4 Régression par discontinuité (Regression discontinuity design) Les groupes sont ventilés pour recevoir ou non un traitement en fonction dun seuil sur une variable mesurable (continue). Par exemple, les personnes qui sont arrêtées au-delà dun seuil dalcool dans le sang ont lobligation de suivre un traitement, et les groupes au-dessous de ce seuil servent de groupe de comparaison (groupe de contrôle). Leffet est mesuré au niveau de la discontinuité entre le groupe traité et le groupe de contrôle (on ne fait pas la différence de moyenne entre deux groupes). Conditions dapplications Le seuil est exogène, non manipulable, et déclenche les actions: majorité absolue \\(\\rightarrow\\) effet de lélection rang du dernier poste offert au concours \\(\\rightarrow\\) effet de lécole seuil de déclenchement dune mesure sociale ou fiscale \\(\\rightarrow\\) effet de la politique, etc Avantages et désavantages Avantages: quand cest bien effectué, la régression par discontinuité permet une estimation non biaisée du traitement. Désavantages: la puissance statistique est bien moindre que dans des essais randomisés contrôlés portant sur le même effectif. Lattention à la puissance statistique est cruciale. les effets sont sans biais uniquement si la forme fonctionnelle entre la variable dassignation et la variable de résultat est bien modélisée, y compris: des relations non-linéaires (augmentation - plateau - augmentation) des interactions (pente coef 1 - seuil - pente coef 2) Estimations économétriques Le modèle linéaire simple (premières estimations) - \\(y_i = \\beta_0 + \\beta_1 x + \\beta_2 (x&gt;seuil) + \\epsilon_i\\), - Leffet causal est mesuré par \\(\\beta_2\\), - Limite suppose que la forme fonctionnelle est la même de part et dautre du seuil (en gros, que le coefficient de la pente soit pas trop différent à gauche et à droite de la discontinuité). Le modèle linéaire avec changement de pente - \\(y_i = \\beta_0 + \\beta_1 x + \\beta_2 (x&gt;seuil) + \\beta_3x \\times (x&gt;seuil) + \\epsilon_i\\) - leffet causal est mesuré par \\(\\beta_2\\). - différence avec modèle linéaire simple: on interagit \\(x\\) pré-seuil et post-seuil. Le modèle linéaire avec changement de forme - attention de bien centrer la variable \\(x\\) autour du seuil ! \\(x&#39; = x-seuil\\), - \\(y_i = \\beta_0 + \\beta_1 x&#39; + \\beta_2 x^{&#39;2} + \\beta_3(x&#39;&gt;0) + \\beta_4x&#39; \\times (x&#39;&gt;0) + \\beta_5 x^{&#39;2} \\times (x&#39;&gt;0) + \\epsilon_i\\), - leffet causal est mesuré par \\(\\beta_3\\). # Regression discontinuity design - je recommende de regarder directement le code du prof et de lancer les commandes. # recentrage de la variable autour du seuil d$var_cont_cent &lt;- d$var_continue - valeur_seuil_au_choix # commande parmi plein d&#39;autres, là c&#39;est toi qui prend l&#39;initiative pr1 &lt;- lm(var_dep ~ var_continue+I(var_continue&gt;=valeur_seuil_au_choix),data=d) # suppose que pente est la même à gauche et à droite du seuil pr1 &lt;- lm(var_dep ~ var_cont_cent+I(var_cont_cent&gt;=0),data=d) pr2 &lt;- lm(var_dep ~ var_continue+I(var_continue&gt;=valeur_seuil_au_choix)+ I((var_continue&gt;=valeur_seuil_au_choix)*var_continue),data=d) # pour cacher les points extrêmes de la base (pour garder la linéarité) pr2b &lt;- lm(var_dep ~ var_continue+I(var_continue&gt;=valeur_seuil_au_choix)+ I((var_continue&gt;=valeur_seuil_au_choix)*var_continue), data=d[d$var_continue&gt;=valeur1 &amp; d$var_continue&lt;=valeur2,]) # pr2 et pr2b supposent que l&#39;effet est linéaire des deux cotés du seuil pr3 &lt;- lm(var_dep ~ var_cont_cent+ I(var_cont_cent&gt;=0)+ I((var_cont_cent&gt;=0)*var_cont_cent)+ I((var_cont_cent&gt;=0)*var_cont_cent2),data=d) 4.2 Variables instrumentales Violation de la 6ème hypothèse des MCO (\\(Cov(x_i,\\epsilon_i \\neq 0)\\) absence de corrélation entre les variables explicatives et le résidu) \\(\\Rightarrow\\) endogénéité. Pb endogénéité peut conduire à se tromper dans linterprétation des paramètres. Variables instrumentales comme technique de correction. 3 problèmes et leurs effets sur les paramètres: Erreur de mesure dune variable explicative: sous-estimation de la valeur absolue du paramètre \\(\\beta_i\\). Variable omise ou hétérogénéité inobservée (corrélée à la variable dépendante et à une autre variable explicative): sur/sous-estimation de la valeur absolue du paramètre. Simultanéité (variable explicative dépend de la variable expliquée): effet plus complexe. Pas dintuition évidente. Solution \\(\\Rightarrow\\) variable instrumentale: trouver une variable instrumentale exogène qui impacte ma variable expliquée \\(y_i\\) uniquement par lintermédiaire de son effet sur la variable explicative \\(x_i\\) suspecte dendogénéité. \\[ instrument \\rightarrow var\\_endo \\rightarrow var\\_dep\\] Sachant que \\(cov(x_{endo},\\epsilon) \\neq 0\\), on introduit une variable instrumentale (\\(z_{inst}\\)) telle que: \\(cov(z_{inst},x_{endo}) \\neq 0\\), et \\(cov(z_{inst},\\epsilon) = 0\\) Vrai modèle7: \\(y = a_{vrai} + b_{vrai} x_{endo} + c_{vrai} x_2 + \\epsilon\\) avec \\(cov(x_{endo},\\epsilon) \\neq 0\\). Première étape: on régresse la variable endogène à la fois sur linstrument et sur les autres variables explicatives. NB: on met toutes les variables explicatives même non pertinentes en première étape. \\[x_{endo} = a_0 + a_1 z_{inst} + a_2 x_2 + \\epsilon_{prem}\\] On récupère de cette première régression \\(x&#39;_{endo}\\), la prédiction de la variable endogène \\(x_{endo}\\): \\[x&#39;_{endo} = a_0 + a_1 z_{inst} + a_2 x_2 = x_{endo} - \\epsilon_{prem}\\] Deuxième étape: on introduit cette prédiction dans la régression à la place \\(x_{endo}\\). \\[y = a_{est} + b_{est} x&#39;_{endo} + c_{est} x_2 + \\epsilon_{deux}\\] Comme \\(z_{inst}\\) et \\(x_2\\) ne sont pas corrélés avec le résidu \\(\\epsilon\\), alors \\(x&#39;_{endo}\\) nest plus corrélé avec \\(\\epsilon\\), lestimateur des variables instrumentales permet destimer sans biais \\(b_{vrai}\\) (et le reste). ## 1ère façon de procéder ## # MCO &quot;naïve&quot; mco &lt;- lm(y ~ x_endo + x1 + x2, data, weights, ...) # Forme réduite - impact des instruments sur variable dépendante (facultatif) fr &lt;- lm(y ~ z1 + z2, data, weights, ...) # Première étape: regression endogene sur autres variables indépendantes et instruments pe &lt;- lm(x_endo ~ x1 + x2 + z1 + z2, data, weights, ...) # Deuxième étape: on introduit la prediction dans la régression &quot;naïve&quot; mco2 &lt;- lm(y ~ pe$fitted.values + x1 + x2, data, weights, ...) ## Deuxième façon de procéder ## # Régression variable instrumentale directe, sans première étape z &lt;- ivreg(y ~ x_endo + x1 + x2 | z1 + z2, data, weights, ...) # Pour voir les coefficients de la première étape z$coefficients1 # Documentation officielle [vérifier si dans cours]: # Note that exogenous regressors have to be included as instruments for themselves. For example, if there is one exogenous regressor ex and one endogenous regressor en with instrument in, the appropriate formula would be ivreg(y ~ ex + en | ex + in) ## Troisième façon de procéder ## # Régression avec instruments, sans effet fixe et sans clusterisation felm(y ~ x_endo + x1 + x2 |0| z1+ z2 |0, data, weights, ...) 4.3 Econométrie des panels Léconométrie des panels rend visible et permet de traiter deux problèmes classiques : 1. Lautocorrélation des résidus, 2. Lhétérogénéité inobservée. Autocorrélation des résidus \\(Cov(\\epsilon_{it},\\epsilon_{it+1}) = 0\\) ? Si je suis sous-payée à la date \\(t\\), je le serai sans doute à la date \\(t+1\\). Hétérogénéité inobservée Si \\(\\epsilon_{it} = v.inobs + e\\) et \\(cov(x_{ik},inobs) \\neq 0 \\Rightarrow\\) biais. Le résidu \\(\\epsilon_{it}\\) peut être réécrit de la manière suivante: \\(\\epsilon_{it} = \\alpha_i + e_{it}\\), comme la somme dune erreur individuelle constante \\(\\alpha_i\\), et \\(e_{it}\\) une erreur temporaire. \\(\\alpha_i\\) peut être vu comme déterminé par les variables inobservées constantes dans le temps. Question: peut-on dire que dans notre vrai modèle cette erreur constante \\(\\alpha_i\\) par individu est indépendante des variables explicatives, soit \\(cov(\\alpha_i,x_k) = 0\\) ? Oui \\(\\rightarrow\\) estimation pooling ou aléatoire est la bonne. Non \\(\\rightarrow\\) hypothèse des MCO \\(cov(\\epsilon_i,x_k) = 0\\) nest pas respectée. Le modèle des MCO nest pas consistant pour estimer les \\(\\beta \\rightarrow\\) modèles à effets fixes ou en différences premières. 4.3.1 Le modèle homogène, ou pooled ou comment résoudre le problème dautocorrélation des résidus. Modèle estimé par les MCO: \\(y_{it}=\\beta_0+\\beta_1 x_{it}+...+\\beta_k x_{kit} + \\epsilon_{it}\\) Le modèle pooled/pooling ou modèle homogène suppose que tous les résidus ont la même variance et quil ny a pas dautocorrélation des résidus. lm(y ~ x1 + x2, data) # ou plm(y ~ x1 + x2, data, index=c(&quot;individu&quot;,&quot;temps&quot;), model = &quot;pooling&quot;) La résolution du problème dautocorrélation des résidus Deux possibilités: Corriger la matrice de variance covariance: estimateur sandwich de Huber-White/ robust standard errors : on corrige les écart-types pour tenir compte de lhétéroscédasticité des résidus (mais pas de lautocorrélation) \\(\\rightarrow\\) hétérogénéité des variances des résidus. correction par cluster / robust clustered standard errors : on corrige les écart-types pour tenir compte à la fois de lhétéroscédasticité des résidus et de leur autocorrélation par cluster (individus, nations, famille) \\(\\rightarrow\\) hétérogénéité des variances des résidus + résidus clusterisés donc homogénéité des variances des résidus au sein des groupes. Modéliser lerreur comme la combinaison dune erreur individuelle fixe et dune erreur temporelle \\(\\Rightarrow\\) modèle à effet aléatoire (random effect). # Là il y a une flopée de méthodes possibles pour corriger les écart-types, mais la manière la plus simple est d&#39;utiliser directement la fonction felm() en précisant la variable de groupe pour obtenir des erreurs-types robustes clusterisées. # Utilise felm(), jdcjdr. felm(y ~ x1 + x2 |0|0| var_cluster_se, data) 4.3.2 Les modèles à effet aléatoire On estime le modèle suivant: \\[y_{it} = \\beta_0 + \\beta_i x_{it} + \\alpha_i + \\epsilon_{it}\\] Deux termes aléatoires \\(\\alpha_i\\) et \\(\\epsilon_{it}\\) qui ont chacun une loi de distribution propre. La technique destimation est celle des moindres carrés généralisés. Lavantage de ce modèle est de permettre une décomposition de la variance des résidus en un facteur individuel (i.e. between, et le cas échéant un facteur temporel) et un facteur idiosyncrasique [big words]. plm(y ~ x1 + x2, data, index, model = &quot;random&quot;) 4.3.3 Le problème dhétérogénéité inobservée Modèle à effet aléatoire : \\[y_{it} = \\beta_0 + \\beta_i x_{it} + \\alpha_i + \\epsilon_{it}\\] Si \\(cov(x_{kit},\\alpha_i) = 0\\), RAS. Mais lerreur individuelle \\(\\alpha_i\\) peut être vue comme le produit de lensemble des variables inobservables invariantes dans le temps. Il nest pas impossible que ces variables inobservables soient corrélées avec les observables. Plutôt que de modéliser les \\(\\alpha_i\\) par un effet aléatoire, on peut les modéliser par un effet fixe. 4.3.3.1 Modèle à effets fixes (fixed effects) Moindre carré à variables dichotomiques : Least square dummy variables ou LSDV. Introduire une variable dichotomique \\(\\alpha_i\\) par individu, possible uniquement si le nombre dindividus est petit (&lt;1000). Modèle within* : modèle (sans constante) où lon centre chaque variable (explicative ou expliquée), càd où lon calcule lécart à la moyenne de la variable pour lindividu. \\[(y_{it} - \\bar y_i) = \\beta_i (x_{it} - \\bar x_i) + (\\epsilon_{it} - \\bar \\epsilon_i)\\] # Estimation des effets fixes ## Avec les effets fixes ## lm(y ~ factor(var_indiv)+x1+x2, data) ## Avec l&#39;estimateur within et le pkg plm ## plm(y ~ x1 + x2, index, model = &quot;within&quot;, data) ## Avec l&#39;estimateur within et le pkg lfe ## felm(y ~ x1 + x2 | var_indiv, data) # en clusterisant les erreurs types par individu felm(y ~ x1 + x2 | var_indiv |0| var_indiv, data) Deux effets fixes: groupes et temps On peut introduire un effet fixe par unité de groupe (e.g. individu, pays, etc.) et un effet fixe par unité de temps. La conjoncture commune capturée par leffet fixe temps: Tout avec des variables dichotomiques8: \\[y_{it}=\\beta_i X_{it} + \\alpha_i + \\delta_t + \\epsilon_{it}\\] Within + variables dichotomiques de temps. plm(y ~ x1 + x2 + factor(var_temps), index, model=&quot;within&quot;, data) Two ways within (formule pour panel cylindrés) plm(y ~ x1 + x2, index, model=&quot;within&quot;, effect=&quot;twoways&quot;, data) # très lent 4.3.3.2 Effets fixes ou effets aléatoires ? Comment choisir? Test de Hausman: on regarde si lestimation par effets fixes produit des résultats significativement différents dun modèle à effets aléatoires. Si le test de Hausman est significatif, on privilégiera les effets fixes. Si le test de Hausman nest pas significatif, on privilégiera les effets aléatoires. regfe &lt;- plm(y ~ x1 + x2, data, index, model = &quot;within&quot;) regre &lt;- plm(y ~ x1 + x2, data, index, model = &quot;random&quot;) phtest(regfe, regre) # p-value significative -&gt; effets fixes. # p-value non significative -&gt; effets aléatoires. 4.3.3.3 Modèles en différences premières (first difference) Expliquer les évolutions par les évolutions (e.g différence revenu entre 1 année, puis 2, puis 5, etc) \\[ (y_{it} - y_{it-1})=\\beta_i (x_{it} - x_{it-1}) + (\\epsilon_{it} - \\epsilon_{it-1})\\] Le choix du retard: \\([t,t-1] \\rightarrow\\) on se focalise sur des variations de court terme, \\([t,t-k] \\rightarrow\\) variations de plus long terme, mais perte deffectifs et de puissance. Différence entre effets fixes &amp; différences premières: panel à deux périodes: modèles équivalents (uniquement lorsque le modèle within comporte aussi un effet fixe temps). panel à plusieurs périodes: le modèle à effets fixes est une moyenne de variations de courtes et de longues périodes. plm(y ~ x1 + x2, index, model = &quot;fd&quot;, data) Effets fixes et différences premières. Remarques Double effet: réduction du problème dautocorrélation et suppression du problème dhétérogénéité inobservée invariante dans le temps. Interprétation: les évolutions expliquent les évolutions. LSDV: variables dichotomiques en très grand nombre (quand on les estime) \\(\\Rightarrow\\) on nimprime généralement pas les résultats. Disparition de la constante en effets fixes mais pas en différences premières. Disparition de toutes les variables constantes dans le temps \\(\\Rightarrow\\) effet contenu dans les variables dichotomiques individuelles. ex: immigration. Parfois, les variables présumées constantes se maintiennent en raison de points aberrants: incohérence de déclaration dune vague à lautre, changement de sexe, diplômes tardifs On peut les introduire malgré tout en les croisant avec une variable temporelle. Linterprétation reste en terme dévolution. 4.3.4 Se concentrer sur la variance interindividuelle et éliminer la variance intra-individuelle Etudier la variance interindividuelle Le modèle à effets fixes ou within permet détudier les évolutions intra-individuelles. La régression homogène (pooled) ou celles à effets aléatoires sont deux manières danalyser une combinaison de variations intra-individuelles et interindividuelles. Peut-on étudier les variations strictement interindividuelles? Oui, de deux façons: Régression toute simple sur une seule période. Régression between. 4.3.4.1 Régression between Régression où lon explique la moyenne par individu de la variable expliquée par les moyennes par individu des variables explicatives. \\[\\bar y = \\beta_0 + \\beta_1 \\bar x_{1i} + ... + \\beta_k \\bar x_{ki} + \\bar \\epsilon_i\\] Cette régression informe sur la variation interindividuelle. Elle peut être considérée comme la bonne régression: Si la variation intra-individuelle dans le temps est négligeable, Si les individus qui connaissent des variations intra-individuelles sont non représentatifs. plm(y ~ x1 + x2, index, model = &quot;between&quot;, data) 4.3.5 Limites des effets fixes On estime les effets sur les individus qui connaissent des évolutions. Si \\(y_i\\) ne change pas en fonction de \\(t\\), alors \\(y_i\\) capturé par leffet fixe \\(a_i\\). Ces évolutions, surtout sur des variables explicatives qualitatives, peuvent être estimées sur des individus rares et très singuliers : biais de sélection Les variables explicatives du changement peuvent être aussi très singulières Ex extrême: changement de sexe. Cela revient à considérer que la différence de résultats entre les sexes est estimée par lévolution de résultat pour ceux qui changent de sexe Ex classique: promotion non-cadre \\(\\rightarrow\\) cadre Les effets fixes reviennent à considérer que les évolutions estimées sur les gens qui changent sont représentatives des différence détat entre gens qui changent pas. Les effets fixes permettent bien de corriger lhétérogénéité inobservée! Mais une partie seulement Lhétérogénéité inobservée invariante dans le temps Maistransforment une régression en une régression en évolution Où un facteur dévolution corrélé aux autres peut être inobservé et conduire à biaiser les autres Où il peut y avoir simultanéité des évolutions de la variable indépendante et des variables indépendantes \\(\\Rightarrow\\) variables instrumentales (si on en trouve) Pas de correction de lhétérogénéité inobservée qui varie dans le temps Ontologie essentialiste Effet individuel: fonds inchangé, permanent, toujours le même. Les évolutions expliquent les évolutions, mais le fixe nexplique pas le changement (sauf spécification contraire). 4.3.6 Extension logistique Probit (effets aléatoires), Logistique (effets fixes, effets aléatoires)9 NB: modèles probit déconseillés avec des effets fixes. Package pglm: binomial models (logit and probit), count models (poisson and negbin) and ordered models (logit and probit). Syntaxe (exemple - actuellement, la méthode within et between ne fonctionne pas pour les modèles logit et probit avec pglm.) pglm(y ~ x1 + x2, index, data, family = &quot;binomial&quot;, model = &quot;random&quot;) Package survival: clogit clogit(y ~ x1 + x2 + strata(var_indiv), data) # je suis pas sûre. de toute façon on n&#39;utilise pas cette méthode. Différent du modèle théorique. A éviter car prend beaucoup de place et de mémoire sur la console R. Logit en panel. "],["an-long.html", "5 Analyses longitudinales 5.1 Pseudo-panels 5.2 Modèles linéaires 5.3 Modèles dynamiques 5.4 Modèles non-linéaires 5.5 Modèles de durée 5.6 Optimal matching", " 5 Analyses longitudinales library(plm) library(survival) library(survminer) 5.1 Pseudo-panels 5.2 Modèles linéaires Modèle de base: \\[y_{i,t} = \\alpha_i + \\beta_i k_{i,t} + \\gamma_i n_{i,t} + \\epsilon_{i,t}\\] lineaire &lt;- lm(y ~ x1 + X2, data=d) Modèle pooled10: \\[y_{i,t} = \\alpha + \\beta k_{i,t} + \\gamma n_{i,t} + \\epsilon_{i,t}\\] pooled &lt;- plm(y ~ x1 + X2, data = d, index = c(&quot;var_individu&quot;,&quot;var_temporelle&quot;), model = &quot;pooling&quot;) Modèle à effet fixe: \\[y_{i,t} = \\alpha_i + \\beta k_{i,t} + \\gamma n_{i,t} + \\epsilon_{i,t}\\] effet_fixe &lt;- plm(y ~ x1 + x2, data, index, model = &quot;within&quot;) # twoways: estimer l&#39;effet fixe individuelle et l&#39;effet fixe temporel model_sete &lt;- plm(y ~ x1 + x2, data, index, model = &quot;within&quot;, effect = &quot;twoways&quot;) Tests ## Test de Fischer ## # Test sur la pertinence de l&#39;hypothèse de l&#39;existence d&#39;un effet fixe individuel (s&#39;applique aussi au modèle pooled). Seule la p-value nous intéresse. coeftest(effet_fixe, vcov. = vcovHC, type = &quot;HC1&quot;) ## pFtest: F-test pour comparer le pouvoir explicatif du modèle A par rapport au modèle B ## pFtest(effet_fixe, pooled) ## Test de Hausman ## phtest(effet_fixe, pooled) 5.3 Modèles dynamiques plm(y ~ lag(y) + lag(X1) + x2, data, index, model = &quot;within&quot;, # ou autre effect = &quot;individual&quot;) # ou autre 5.4 Modèles non-linéaires 5.5 Modèles de durée 5.6 Optimal matching Moindres Carrés Ordinaires askip??? "],["an-res.html", "6 Analyses de réseaux 6.1 Bases de la théorie 6.2 Structures locales 6.3 Connectivité 6.4 Mesures basiques de cohésion 6.5 Mesures de diversité 6.6 Mesures de centralité", " 6 Analyses de réseaux library(igraph) graph &lt;- graph.data.frame(links,nodes,directed=TRUE) graph &lt;- graph.adjacency(df,mode=&quot;directed&quot;) 6.1 Bases de la théorie Graphe : ensemble dunités (sommets) connectées par une ou plusieurs relations (arêtes) Sommets (noeuds, vertices) : unités Arêtes (liens, edges) : relations Réseaux complets v réseaux personnels (réseaux égo-centrés) # Taille du réseau = nombre de noeuds length(V(graph)) # Nombre de liens length(E(graph)) 6.1.1 Matrices Ou comment stocker ses données avant de les traiter. dadjacence matrice symétrique avec liens non-orientés (du coup liens réciproques) Carrée \\(n* n\\) Edge list : chaque paire de noeuds connectés sur une ligne dune table Node list : chaque ligne représente les liens dun noeud vers tous les autres 6.1.2 Liens Orientés Réciproques Non-orientés 6.2 Structures locales Ou comment un réseau sorganise. Noeuds isolés Dyades Réciprocité Triades (liens orientés) Empty One-edge Two-path Triangle Triades (liens non-orientés) Intransitive : liens bilatéraux uniquement Transitive : lami de mon ami est mon ami Trois-cycles : forme déchange généralisé Clique : sous-ensemble de noeuds où toutes les paires de noeuds existants sont connectés. Modularité : mesure segmentation dun réseau en modules. Réseau à modularité élevée a densité élevée entre les noeusd qui font partie dun même module, densité faible entre noeuds appartenant à modules différents. # Trois algorithmes de modularité wtc &lt;- walktrap.community(graph) grd &lt;- fastgreedy.community(graph) spn &lt;- spinglass.community(graph) # Indicateur de modularité pour le réseau modularity(graph, membership(wtc)) modularity(graph, membership(grd)) modularity(graph, membership(spn)) 6.3 Connectivité Chaîne/walk : parcours sur graphe non orienté allant dun noeud à un autre en empruntant des arêtes (liens) Chemin : chaîne mais pour un graphe orienté Chaîne/chemin élementaire (path) si chaque noeud y apparaît au plus une fois. Géodésique (geodesic) : chaine/chemin élémentaire la/le plus court(e) (shortest path) entre deux noeuds. Cycle : départ et arrivée de la chaine/chemin élémentaire est le même noeud. Graphe connexe (connected) : chemin ou chaîne entre toute paire de noeuds. Composante : sous-graphe maximalement connecté. Distance (géodésique) : nb de pas (plus courts chemins) entre un noeud et lautre. noeuds connectés ont distance 1 noeuds dans composantes différentes ont distance infinie Diamètre : distance la plus longue entre deux noeuds. Average path length : distance moyenne entre toutes les pairs de noeuds dans un réseau (moins sensible à des outiliers que le diamètre). Eccentricité : distance depuis un noeud de départ vers le noeud le plus loin dans le réseau. Rayon : eccentricité minimale des noeuds. La plus petite distance à laquelle puisse se trouver un noeud de tous les autres (infini si graphe est non connecté) shortest.paths(graph, algorithm=&quot;unweighted&quot;) # Shortest path entre deux noeuds get.shortest.paths(graph, V(graph)[name==&quot;name1&quot;], V(graph)[name==&quot;name2&quot;], mode=&quot;all&quot;, output=&quot;both&quot;) # Distance moyenne entre les noeuds average.path.length(graph) # Diamètre diameter(graph) # Eccentricité eccentricity(graph) # Rayon : eccentricité la plus faible radius(graph) 6.4 Mesures basiques de cohésion Tous les noeuds sont-ils liés entre eux? Quels types de liens existent dans le réseau et dans quelle quantité? Transitivité : \\(\\frac{nombre.triades.transitives}{nombre.triades}\\) égal à 1 si tous les noeuds sont liés à tous les autres noeuds (connectivité complète) Excède rarement 0.2 dans réseaux aléatoires. Souvent compris entre 0.3 et 0.6 dans les réseaux empiriques. Densité : \\(\\frac{nombre.liens.existants}{nb.liens.pouvant.existés}\\) \\(\\frac{L}{(n*(n-1))}\\) liens orientés \\(\\frac{L}{\\frac{(n*(n-1))}{2}}\\) liens non-orientés Coefficient de clustering : mesure de cohésion dans le voisinage dun noeud (combien de mes amis sont amis entre eux). 2 mesures : Mesure locale : on mesure dabord pour chaque noeud i, le \\(Cl_{i}\\) ensuite on prend la moyenne \\(\\sum_{i = 1}^{n}\\)\\(\\frac{Cl_{i}}{n}\\). Tends to 1. Mesure globale : \\(\\sum_{i=1}^{n}\\)\\(\\frac{nombre.liens.existants.entre.amis.de.i}{nombre.liens.possibles.entre.amis.de.i}\\). Tends to 0 =&gt; transitivité. # Transitivité du réseau transitivity(graph) # Transitivité d&#39;un noeud transitivity(graph,type=&quot;local&quot;) # Densité du réseau graph.density(graph) # Nombre d&#39;îles, i.e. clusters clusters(graph) 6.5 Mesures de diversité Tous mes groupes sont-ils représentés proportionnellement dans mon réseau? Deux familles de mesures: proportion (ou pourcentage) dune catégorie sur la totalité hétérogénéité (variance, écart-type, IQV) Indice de diversité de Blau \\(\\in[0;\\frac{k-1}{k}]\\) Une seule catégorie représentée =&gt; toutes les catégories représentées équitablement (utile si plus de 2 catégories - variante de lindice Herfindahl-Hirschmann (HHI)) Indice de variation qualitative, IQV \\(\\in[0;1]\\) Une seule catégorie représentée =&gt; toutes les catégories représentées équitablement (un indice POUR CHAQUE ATTRIBUT dintérêt) get.Blau.index &lt;- function(x, type) { x &lt;- factor(x, levels = type); return(1 - sum(prop.table(table(x))^2))} # Indice de diversité de Blau qualif_blau &lt;- get.Blau.index(as.factor(V(graph)$variable)) # on applique la fonction de l&#39;IB à la variable x qualif_blau # Indice de variation qualitative qualif_iqv = qualif_blau / (1 - (1 / length(levels(as.factor(Proportions$Var1))))) qualif_iqv # IQV de Qualification!!! # HHI qui est égal à 1 - qualif_blau qualif_hhi &lt;- 1 - qualif_blau qualif_hhi 6.6 Mesures de centralité Y a-t-il un noeud ou groupe de noeuds qui a une plus grande importance/qui est le plus relié dans le réseau? Centralité de degré (degree) noeuds les plus actifs (les plus connectées, qui sont liés à un plus grand nombre de noeuds) \\(C_D(i) =\\) \\(\\sum_{j=1}^{n}x_{ij} = \\sum_{j=1}^{n} x_{ji}\\) mesure normalisée: \\(C&#39;_D(i)=\\frac{\\sum_{j=1}^{n}x_{ij}}{n-1}\\) centralité de demi-degré pour graphe orienté. Indicateur de position sociale. Extérieur (outdegree) = nb liens sortants (e.g: demander beaucoup de conseils) Intérieur (indegree) = nb liens entrants (e.g: recevoir beaucoup de demandes de conseils) Centralité dintermédiarité (betweenness) position stratégique, entre différentes parties du réseau (e.g être le lien entre deux parties non connectées) nb de plus courts chemins entre toute paire dacteurs k et j, et on prend ceux qui passent par i \\(C_B(i)=\\)\\(\\sum_{jk}\\)\\(\\frac{s_{kij}}{s_{kj}}\\) Centralité de proximité (closeness). Un peu comme centralité de degré, mais noeuds sont pas aussi centraux. Centralité de vecteur propre (eigenvector). Être connecté aux autres noeuds les plus connectés. Centralité dans un réseau : se calcule pour chaque noeud dans un réseau (devient un attribut du noeud) [NB: soit centra de proxi, soit centra de degré, soit intermédiarité, etc]. Dans quelle mesure le réseau est dominé par un noeud central (ou peu de noeuds centraux) ? On compare la centralité du noeud le plus central à la centralité des autres noeuds. Au niveau du réseau dans son ensemble, on peut regarder: La distribution des centralités des noeuds; Des indicateurs de centralisation agrégés. mesure de centralisation de Freeman # Centralité de degré degree(graph,mode=&quot;all&quot;) degree(graph,mode=&quot;in&quot;) degree(graph,mode=&quot;out&quot;) # Centralité d&#39;intermédiarité betweenness(graph,directed=TRUE) # Centralité de proximité # il faut d&#39;abord enlever les noeuds isolés, sinon le calcul ne marche pas Isolated = which(degree(graph)==0) graph2 = delete.vertices(graph, Isolated) # on construit un nouveau graphe en enlevant Isolated # on calcule la proximité sur ce graphe close_graph &lt;- closeness(graph2, mode=&#39;all&#39;, normalized = FALSE) # pour obtenir la valeur normalisée, la commande est : close_graph2 &lt;- closeness(graph2, mode=&#39;all&#39;, normalized = TRUE) # Centralité de vecteur propre eigen_centrality(graph, scale = TRUE, weights = NULL) "]]
